{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero or negative values in 'LargestPropertyUseTypeGFA': 0\n",
      "Some scaled values: 0    0.726587\n",
      "1    0.664610\n",
      "2    3.242804\n",
      "3    0.297358\n",
      "4    1.117587\n",
      "Name: LargestPropertyUseTypeGFA, dtype: float64\n",
      "Cross Validation Score: 0.5258021032620898\n",
      "Best Params: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 10}\n",
      "Best Score: 0.7402157485522256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['best_scaler.pkl']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "\n",
    "# Ignorer les avertissements\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv('2016_Building_Energy_Benchmarking.csv')\n",
    "\n",
    "# Préparation des données\n",
    "features = df[['LargestPropertyUseTypeGFA', 'PrimaryPropertyType']].copy()\n",
    "targets = df[['SiteEnergyUse(kBtu)', 'TotalGHGEmissions']].copy()\n",
    "\n",
    "# Save the original columns before one-hot encoding\n",
    "original_columns = features.columns.tolist()\n",
    "\n",
    "# Transformer les variables catégoriques en variables numériques\n",
    "features = pd.get_dummies(features, columns=['PrimaryPropertyType'])\n",
    "\n",
    "# Drop les NaN values du dataset\n",
    "df = pd.concat([features, targets], axis=1).dropna()\n",
    "features = df.drop(['SiteEnergyUse(kBtu)', 'TotalGHGEmissions'], axis=1)\n",
    "targets = df[['SiteEnergyUse(kBtu)', 'TotalGHGEmissions']]\n",
    "\n",
    "# Save the columns of the DataFrame\n",
    "with open('columns.pkl', 'wb') as f:\n",
    "    pickle.dump(features.columns.tolist(), f)\n",
    "\n",
    "# Save the original columns before one-hot encoding\n",
    "with open('original_columns.pkl', 'wb') as f:\n",
    "    pickle.dump(original_columns, f)\n",
    "\n",
    "# Transformation logarithmique de la variable 'LargestPropertyUseTypeGFA'\n",
    "features['LargestPropertyUseTypeGFA'] = np.log(features['LargestPropertyUseTypeGFA'])\n",
    "\n",
    "# Rescaling des variables numériques\n",
    "scaler = StandardScaler()\n",
    "features[['LargestPropertyUseTypeGFA']] = scaler.fit_transform(features[['LargestPropertyUseTypeGFA']])\n",
    "\n",
    "# Diviser les données en ensemble d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Définir les paramètres pour la recherche de grille\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(model, param_distributions=rf_param_grid, n_iter=20, cv=5)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "scores = [r2_score(y_test[column], predictions[:, i]) for i, column in enumerate(y_test.columns)]\n",
    "best_score = np.mean(scores)\n",
    "\n",
    "# Cross validation score\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5)\n",
    "cv_score_mean = np.mean(cv_scores)\n",
    "\n",
    "print(f\"Cross Validation Score: {cv_score_mean}\")\n",
    "print(f\"Best Params: {best_params}\")\n",
    "print(f\"Best Score: {best_score}\")\n",
    "\n",
    "# Enregistrer le modèle dans un fichier .pkl\n",
    "with open('best_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "# Enregistrer le scaler dans un fichier .pkl\n",
    "joblib.dump(scaler, 'best_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6747639.74     205.25]]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with open('best_model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "scaler = joblib.load('best_scaler.pkl')\n",
    "\n",
    "# Load the model's columns\n",
    "with open('columns.pkl', 'rb') as f:\n",
    "    model_columns = pickle.load(f)\n",
    "\n",
    "# Create a new row of data with all zeros\n",
    "new_data = pd.DataFrame(columns=model_columns)\n",
    "new_data.loc[0] = [0 for _ in range(len(model_columns))]\n",
    "\n",
    "# Fill in the data for your prediction\n",
    "new_data['PrimaryPropertyType_Hotel'] = 1\n",
    "\n",
    "# The initial value before transformations\n",
    "initial_value = 88434  # change this to your actual value\n",
    "\n",
    "# Apply inverse transformations\n",
    "log_value = np.log(initial_value)\n",
    "scaled_value = scaler.transform([[log_value]])[0][0]\n",
    "\n",
    "new_data['LargestPropertyUseTypeGFA'] = scaled_value\n",
    "\n",
    "predictions = model.predict(new_data)\n",
    "\n",
    "# Set options for printing\n",
    "np.set_printoptions(suppress=True, precision=2)\n",
    "\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero or negative values in 'LargestPropertyUseTypeGFA': 0\n",
      "Some scaled values: 0    0.726587\n",
      "1    0.664610\n",
      "2    3.242804\n",
      "3    0.297358\n",
      "4    1.117587\n",
      "Name: LargestPropertyUseTypeGFA, dtype: float64\n",
      "Cross Validation Score: 0.4922524368709686\n",
      "Best Params: {'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 20}\n",
      "Best Score: 0.7237203548715406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['best_scaler.pkl']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "\n",
    "# Ignorer les avertissements\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv('2016_Building_Energy_Benchmarking.csv')\n",
    "\n",
    "# Préparation des données\n",
    "features = df[['LargestPropertyUseTypeGFA', 'PrimaryPropertyType']].copy()\n",
    "targets = df[['SiteEnergyUse(kBtu)', 'TotalGHGEmissions']].copy()\n",
    "\n",
    "# Save the original columns before one-hot encoding\n",
    "original_columns = features.columns.tolist()\n",
    "\n",
    "# Transformer les variables catégoriques en variables numériques\n",
    "features = pd.get_dummies(features, columns=['PrimaryPropertyType'])\n",
    "\n",
    "# Drop les NaN values du dataset\n",
    "df = pd.concat([features, targets], axis=1).dropna()\n",
    "features = df.drop(['SiteEnergyUse(kBtu)', 'TotalGHGEmissions'], axis=1)\n",
    "targets = df[['SiteEnergyUse(kBtu)', 'TotalGHGEmissions']]\n",
    "\n",
    "# Save the columns of the DataFrame\n",
    "with open('columns.pkl', 'wb') as f:\n",
    "    pickle.dump(features.columns.tolist(), f)\n",
    "\n",
    "# Save the original columns before one-hot encoding\n",
    "with open('original_columns.pkl', 'wb') as f:\n",
    "    pickle.dump(original_columns, f)\n",
    "\n",
    "# Check for any zero or negative values in 'LargestPropertyUseTypeGFA'\n",
    "print(\"Zero or negative values in 'LargestPropertyUseTypeGFA':\", features['LargestPropertyUseTypeGFA'].le(0).sum())\n",
    "\n",
    "# Transformation logarithmique de la variable 'LargestPropertyUseTypeGFA'\n",
    "features['LargestPropertyUseTypeGFA'] = np.log(features['LargestPropertyUseTypeGFA'])\n",
    "\n",
    "# Rescaling des variables numériques\n",
    "scaler = StandardScaler()\n",
    "features[['LargestPropertyUseTypeGFA']] = scaler.fit_transform(features[['LargestPropertyUseTypeGFA']])\n",
    "\n",
    "# Print some scaled values for inspection\n",
    "print(\"Some scaled values:\", features['LargestPropertyUseTypeGFA'].head())\n",
    "\n",
    "# Diviser les données en ensemble d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "# Définir les paramètres pour la recherche de grille\n",
    "et_param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "model = ExtraTreesRegressor(random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(model, param_distributions=et_param_grid, n_iter=100, cv=5)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "scores = [r2_score(y_test[column], predictions[:, i]) for i, column in enumerate(y_test.columns)]\n",
    "best_score = np.mean(scores)\n",
    "\n",
    "# Cross validation score\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5)\n",
    "cv_score_mean = np.mean(cv_scores)\n",
    "\n",
    "print(f\"Cross Validation Score: {cv_score_mean}\")\n",
    "print(f\"Best Params: {best_params}\")\n",
    "print(f\"Best Score: {best_score}\")\n",
    "\n",
    "# Enregistrer le modèle dans un fichier .pkl\n",
    "with open('best_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "# Enregistrer le scaler dans un fichier .pkl\n",
    "joblib.dump(scaler, 'best_scaler.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6370078.45     175.98]]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with open('best_model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "scaler = joblib.load('best_scaler.pkl')\n",
    "\n",
    "# Load the model's columns\n",
    "with open('columns.pkl', 'rb') as f:\n",
    "    model_columns = pickle.load(f)\n",
    "\n",
    "# Create a new row of data with all zeros\n",
    "new_data = pd.DataFrame(columns=model_columns)\n",
    "new_data.loc[0] = [0 for _ in range(len(model_columns))]\n",
    "\n",
    "# Fill in the data for your prediction\n",
    "new_data['PrimaryPropertyType_Hotel'] = 1\n",
    "\n",
    "# The initial value before transformations\n",
    "initial_value = 88434  # change this to your actual value\n",
    "\n",
    "# Apply inverse transformations\n",
    "log_value = np.log(initial_value)\n",
    "scaled_value = scaler.transform([[log_value]])[0][0]\n",
    "\n",
    "new_data['LargestPropertyUseTypeGFA'] = scaled_value\n",
    "\n",
    "predictions = model.predict(new_data)\n",
    "\n",
    "# Set options for printing\n",
    "np.set_printoptions(suppress=True, precision=2)\n",
    "\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Scores:\n",
      "SiteEnergyUse(kBtu): 0.6284824756974252\n",
      "TotalGHGEmissions: 0.8867741478990165\n",
      "RMSE Scores:\n",
      "SiteEnergyUse(kBtu): 12351625.219564518\n",
      "TotalGHGEmissions: 192.43179331187875\n",
      "Cross Validation Score: 0.5237068864600779\n",
      "Best Params: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 40}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['best_scaler.pkl']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import warnings\n",
    "\n",
    "# Ignorer les avertissements\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv('2016_Building_Energy_Benchmarking.csv')\n",
    "\n",
    "# Préparation des données\n",
    "features = df[['LargestPropertyUseTypeGFA', 'PrimaryPropertyType']].copy()\n",
    "targets = df[['SiteEnergyUse(kBtu)', 'TotalGHGEmissions']].copy()\n",
    "\n",
    "# Save the original columns before one-hot encoding\n",
    "original_columns = features.columns.tolist()\n",
    "\n",
    "# Transformer les variables catégoriques en variables numériques\n",
    "features = pd.get_dummies(features, columns=['PrimaryPropertyType'])\n",
    "\n",
    "# Drop les NaN values du dataset\n",
    "df = pd.concat([features, targets], axis=1).dropna()\n",
    "features = df.drop(['SiteEnergyUse(kBtu)', 'TotalGHGEmissions'], axis=1)\n",
    "targets = df[['SiteEnergyUse(kBtu)', 'TotalGHGEmissions']]\n",
    "\n",
    "# Save the columns of the DataFrame\n",
    "with open('columns.pkl', 'wb') as f:\n",
    "    pickle.dump(features.columns.tolist(), f)\n",
    "\n",
    "# Save the original columns before one-hot encoding\n",
    "with open('original_columns.pkl', 'wb') as f:\n",
    "    pickle.dump(original_columns, f)\n",
    "\n",
    "# Transformation logarithmique de la variable 'LargestPropertyUseTypeGFA'\n",
    "features['LargestPropertyUseTypeGFA'] = np.log(features['LargestPropertyUseTypeGFA'])\n",
    "\n",
    "# Rescaling des variables numériques\n",
    "scaler = StandardScaler()\n",
    "features[['LargestPropertyUseTypeGFA']] = scaler.fit_transform(features[['LargestPropertyUseTypeGFA']])\n",
    "\n",
    "# Diviser les données en ensemble d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "# Définir les paramètres pour la recherche de grille\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(model, param_distributions=rf_param_grid, n_iter=20, cv=5)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "r2_scores = [r2_score(y_test[column], predictions[:, i]) for i, column in enumerate(y_test.columns)]\n",
    "rmse_scores = [mean_squared_error(y_test[column], predictions[:, i], squared=False) for i, column in enumerate(y_test.columns)]\n",
    "\n",
    "# Print R2 scores\n",
    "print(\"R2 Scores:\")\n",
    "for column, score in zip(y_test.columns, r2_scores):\n",
    "    print(f\"{column}: {score}\")\n",
    "\n",
    "# Print RMSE scores\n",
    "print(\"RMSE Scores:\")\n",
    "for column, score in zip(y_test.columns, rmse_scores):\n",
    "    print(f\"{column}: {score}\")\n",
    "\n",
    "# Cross validation score\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5)\n",
    "cv_score_mean = np.mean(cv_scores)\n",
    "\n",
    "print(f\"Cross Validation Score: {cv_score_mean}\")\n",
    "print(f\"Best Params: {best_params}\")\n",
    "\n",
    "# Enregistrer le modèle dans un fichier .pkl\n",
    "with open('best_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "# Enregistrer le scaler dans un fichier .pkl\n",
    "joblib.dump(scaler, 'best_scaler.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model average R2 Score (Testing): 0.7720214034846744\n",
      "         Model               Target  R2 Score (Training)  RMSE (Training)  \\\n",
      "0  Extra Trees  SiteEnergyUse(kBtu)             0.997224     1.158422e+06   \n",
      "1  Extra Trees    TotalGHGEmissions             0.993794     4.184152e+01   \n",
      "2  Extra Trees  SiteEnergyUse(kBtu)             0.166481     2.092170e+07   \n",
      "3  Extra Trees    TotalGHGEmissions             0.325687     4.384207e+02   \n",
      "4  Extra Trees  SiteEnergyUse(kBtu)             0.586319     9.116150e+06   \n",
      "5  Extra Trees    TotalGHGEmissions             0.768881     2.092103e+02   \n",
      "6  Extra Trees  SiteEnergyUse(kBtu)             0.978454     3.166246e+06   \n",
      "7  Extra Trees    TotalGHGEmissions             0.992066     4.064510e+01   \n",
      "8  Extra Trees  SiteEnergyUse(kBtu)             0.365620     1.280530e+07   \n",
      "9  Extra Trees    TotalGHGEmissions             0.375098     4.086844e+02   \n",
      "\n",
      "   R2 Score (Testing)  RMSE (Testing)  \n",
      "0            0.645341    1.206812e+07  \n",
      "1            0.898701    1.820144e+02  \n",
      "2            0.446299    1.162740e+07  \n",
      "3            0.576931    3.652718e+02  \n",
      "4            0.082662    3.753308e+07  \n",
      "5            0.072039    8.034732e+02  \n",
      "6            0.416003    1.679865e+07  \n",
      "7            0.316258    6.522149e+02  \n",
      "8            0.102876    3.425729e+07  \n",
      "9            0.366317    4.947286e+02  \n",
      "\n",
      "Best model: Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('log',\n",
      "                                                  FunctionTransformer(func=<ufunc 'log'>,\n",
      "                                                                      validate=True),\n",
      "                                                  ['LargestPropertyUseTypeGFA']),\n",
      "                                                 ('one_hot',\n",
      "                                                  OneHotEncoder(drop='first',\n",
      "                                                                sparse=False),\n",
      "                                                  ['PrimaryPropertyType'])])),\n",
      "                ('scaler', StandardScaler()),\n",
      "                ('model',\n",
      "                 MultiOutputRegressor(estimator=ExtraTreesRegressor(max_depth=5,\n",
      "                                                                    n_estimators=50)))])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['logbest_scaler.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import ShuffleSplit, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_csv('C:/Users/loren/OneDrive/Documents/GitHub/co2/2016_Building_Energy_Benchmarking.csv')\n",
    "\n",
    "X = df[['LargestPropertyUseTypeGFA', 'PrimaryPropertyType']].copy()\n",
    "y = df[['SiteEnergyUse(kBtu)', 'TotalGHGEmissions']].copy()\n",
    "X[['LargestPropertyUseTypeGFA']] += 0.01\n",
    "\n",
    "df = pd.concat([X, y], axis=1).dropna()\n",
    "\n",
    "\n",
    "X = df.drop(['SiteEnergyUse(kBtu)', 'TotalGHGEmissions'], axis=1)\n",
    "# Utiliser une constante plus grande pour garantir que toutes les valeurs soient positives avant la transformation logarithmique\n",
    "y = np.log(df[['SiteEnergyUse(kBtu)', 'TotalGHGEmissions']] + 1) \n",
    "\n",
    "log_transformer = FunctionTransformer(np.log, validate=True)\n",
    "one_hot_encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "scaler = StandardScaler()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('log', log_transformer, ['LargestPropertyUseTypeGFA']),\n",
    "        ('one_hot', one_hot_encoder, ['PrimaryPropertyType'])\n",
    "    ])\n",
    "\n",
    "models = [\n",
    "    (\"Extra Trees\", ExtraTreesRegressor(), {'model__estimator__n_estimators': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400, 500],\n",
    "                                            'model__estimator__max_depth': [None, 5, 10, 15, 20, 25, 30]})\n",
    "]\n",
    "\n",
    "best_model = None\n",
    "best_scaler = None\n",
    "best_avg_test_score = -np.inf \n",
    "\n",
    "results = []\n",
    "n_splits = 5\n",
    "test_size = 0.2\n",
    "shuffle_split = ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=42)\n",
    "\n",
    "for name, model, params in models:\n",
    "    for train_index, test_index in shuffle_split.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        pipeline = Pipeline(steps=[('preprocessor', preprocessor), \n",
    "                                   ('scaler', scaler), \n",
    "                                   ('model', MultiOutputRegressor(model))])\n",
    "\n",
    "        random_search = RandomizedSearchCV(pipeline, param_distributions=params, n_iter=10, cv=5, random_state=42)\n",
    "        random_search.fit(X_train, y_train)\n",
    "        best_model = random_search.best_estimator_\n",
    "\n",
    "        y_train_pred = best_model.predict(X_train)\n",
    "        y_test_pred = best_model.predict(X_test)\n",
    "        \n",
    "        y_train_pred_exp = np.exp(y_train_pred) - 0.01\n",
    "        y_test_pred_exp = np.exp(y_test_pred) - 0.01\n",
    "        y_train_exp = np.exp(y_train) - 0.01\n",
    "        y_test_exp = np.exp(y_test) - 0.01\n",
    "\n",
    "        for i, target in enumerate(y.columns):\n",
    "            r2_train = r2_score(y_train_exp.iloc[:, i], y_train_pred_exp[:, i])\n",
    "            r2_test = r2_score(y_test_exp.iloc[:, i], y_test_pred_exp[:, i])\n",
    "            rmse_train = mean_squared_error(y_train_exp.iloc[:, i], y_train_pred_exp[:, i], squared=False)\n",
    "            rmse_test = mean_squared_error(y_test_exp.iloc[:, i], y_test_pred_exp[:, i], squared=False)\n",
    "\n",
    "            results.append([name, target, r2_train, rmse_train, r2_test, rmse_test])\n",
    "\n",
    "        avg_test_score = np.mean([result[-2] for result in results[-2:]])\n",
    "\n",
    "        if avg_test_score > best_avg_test_score:\n",
    "            best_avg_test_score = avg_test_score\n",
    "            best_model = random_search.best_estimator_\n",
    "            best_scaler = scaler\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Target', 'R2 Score (Training)', 'RMSE (Training)', 'R2 Score (Testing)', 'RMSE (Testing)'])\n",
    "\n",
    "print(f\"Best model average R2 Score (Testing): {best_avg_test_score}\")\n",
    "print(results_df)\n",
    "print(f\"\\nBest model: {best_model}\")\n",
    "\n",
    "joblib.dump(best_model, 'logbest_model.pkl')\n",
    "joblib.dump(best_scaler, 'logbest_scaler.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
