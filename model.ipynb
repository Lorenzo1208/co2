{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.4083778826430925\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv('2016_Building_Energy_Benchmarking.csv')\n",
    "\n",
    "# Préparation des données\n",
    "features = df[['LargestPropertyUseTypeGFA', 'BuildingType', 'PrimaryPropertyType', 'PropertyGFATotal', 'PropertyGFABuilding(s)']].copy()\n",
    "target = df['SiteEnergyUse(kBtu)'].copy()\n",
    "\n",
    "# Transformer les variables catégoriques en variables numériques\n",
    "features = pd.get_dummies(features, columns=['BuildingType', 'PrimaryPropertyType'])\n",
    "\n",
    "# Drop les NaN values du dataset\n",
    "df = pd.concat([features, target], axis=1).dropna()\n",
    "features = df.drop('SiteEnergyUse(kBtu)', axis=1)\n",
    "target = df['SiteEnergyUse(kBtu)']\n",
    "\n",
    "# Rescaling des variables numériques\n",
    "scaler = StandardScaler()\n",
    "features[['PropertyGFATotal', 'LargestPropertyUseTypeGFA']] = scaler.fit_transform(features[['PropertyGFATotal', 'LargestPropertyUseTypeGFA']])\n",
    "\n",
    "# Sélectionner uniquement les 5 premières caractéristiques\n",
    "selected_features = features.iloc[:, :5]\n",
    "\n",
    "# Diviser les données en ensemble d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(selected_features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Définir le modèle à utiliser\n",
    "model = Lasso(alpha=1084.7831)\n",
    "\n",
    "# Entraîner le modèle\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire sur l'ensemble de test\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculer le R2 Score\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print('R2 Score:', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Enregistrer le modèle dans un fichier .pkl\n",
    "with open('model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: 0.4289976236218547\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv('2016_Building_Energy_Benchmarking.csv')\n",
    "\n",
    "# Préparation des données\n",
    "features = df[['LargestPropertyUseTypeGFA']].copy()\n",
    "target = df['SiteEnergyUse(kBtu)'].copy()\n",
    "\n",
    "#SiteEnergyUse(kBtu)\n",
    "#TotalGHGEmissions\n",
    "# Drop les NaN values du dataset\n",
    "df = pd.concat([features, target], axis=1).dropna()\n",
    "features = df.drop('SiteEnergyUse(kBtu)', axis=1)\n",
    "target = df['SiteEnergyUse(kBtu)']\n",
    "\n",
    "# Rescaling des variables numériques\n",
    "scaler = StandardScaler()\n",
    "features[['LargestPropertyUseTypeGFA']] = scaler.fit_transform(features[['LargestPropertyUseTypeGFA']])\n",
    "\n",
    "# Sélectionner uniquement les 5 premières caractéristiques\n",
    "selected_features = features.iloc[:, :5]\n",
    "\n",
    "# Diviser les données en ensemble d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(selected_features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Définir le modèle à utiliser\n",
    "model = Lasso(alpha=1084.7831)\n",
    "\n",
    "# Entraîner le modèle\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prédire sur l'ensemble de test\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculer le R2 Score\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print('R2 Score:', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de cas où LargestPropertyUseTypeGFA est supérieur à PropertyGFATotal : 339\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv('2016_Building_Energy_Benchmarking.csv')\n",
    " \n",
    "# Compter le nombre de cas où LargestPropertyUseTypeGFA est supérieur à PropertyGFATotal\n",
    "count = df[df['LargestPropertyUseTypeGFA'] > df['PropertyGFATotal']].shape[0]\n",
    "\n",
    "print(\"Nombre de cas où LargestPropertyUseTypeGFA est supérieur à PropertyGFATotal :\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de cas où LargestPropertyUseTypeGFA est supérieur à PropertyGFATotal : 1279\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv('2016_Building_Energy_Benchmarking.csv')\n",
    "\n",
    "# Compter le nombre de cas où LargestPropertyUseTypeGFA est supérieur à PropertyGFATotal\n",
    "count = df[df['LargestPropertyUseTypeGFA'] == df['PropertyGFATotal']].shape[0]\n",
    "\n",
    "print(\"Nombre de cas où LargestPropertyUseTypeGFA est supérieur à PropertyGFATotal :\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: Lasso()\n",
      "Best R2 Score: 0.6758632370613493\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#SiteEnergyUse(kBtu)\n",
    "#TotalGHGEmissions\n",
    "\n",
    "# Charger les données\n",
    "# Charger les données\n",
    "df = pd.read_csv('2016_Building_Energy_Benchmarking.csv')\n",
    "\n",
    "# Préparation des données\n",
    "features = df[['LargestPropertyUseTypeGFA', 'BuildingType', 'PrimaryPropertyType', 'PropertyGFATotal', 'PropertyGFABuilding(s)']].copy()\n",
    "target = df['TotalGHGEmissions'].copy()\n",
    "\n",
    "# Transformer les variables catégoriques en variables numériques\n",
    "features = pd.get_dummies(features, columns=['BuildingType', 'PrimaryPropertyType'])\n",
    "\n",
    "# Drop les NaN values du dataset\n",
    "df = pd.concat([features, target], axis=1).dropna()\n",
    "features = df.drop('TotalGHGEmissions', axis=1)\n",
    "target = df['TotalGHGEmissions']\n",
    "\n",
    "# Rescaling des variables numériques\n",
    "scaler = StandardScaler()\n",
    "features[['LargestPropertyUseTypeGFA', 'PropertyGFATotal', 'PropertyGFABuilding(s)']] = scaler.fit_transform(features[['LargestPropertyUseTypeGFA', 'PropertyGFATotal', 'PropertyGFABuilding(s)']])\n",
    "\n",
    "# Diviser les données en ensemble d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Liste des modèles à tester avec différents hyperparamètres\n",
    "models = [\n",
    "    Lasso()\n",
    "]\n",
    "\n",
    "best_model = None\n",
    "best_r2 = -float('inf')\n",
    "\n",
    "# Entraîner et évaluer les modèles\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    \n",
    "    if r2 > best_r2:\n",
    "        best_r2 = r2\n",
    "        best_model = model\n",
    "\n",
    "print('Best Model:', best_model)\n",
    "print('Best R2 Score:', best_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Enregistrer le modèle dans un fichier .pkl\n",
    "with open('modelemission.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "import joblib\n",
    "joblib.dump(scaler, 'scaleremission.pkl')\n",
    "with open('columns_emission.pkl', 'wb') as f:\n",
    "    pickle.dump(features.columns, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model: Lasso()\n",
      "Best R2 Score: 0.5716105170328534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.685e+16, tolerance: 1.297e+14\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#SiteEnergyUse(kBtu)\n",
    "#TotalGHGEmissions\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv('2016_Building_Energy_Benchmarking.csv')\n",
    "\n",
    "# Préparation des données\n",
    "features = df[['LargestPropertyUseTypeGFA', 'BuildingType', 'PrimaryPropertyType', 'PropertyGFATotal', 'PropertyGFABuilding(s)']].copy()\n",
    "target = df['SiteEnergyUse(kBtu)'].copy()\n",
    "\n",
    "# Transformer les variables catégoriques en variables numériques\n",
    "features = pd.get_dummies(features, columns=['BuildingType', 'PrimaryPropertyType'])\n",
    "\n",
    "# Drop les NaN values du dataset\n",
    "df = pd.concat([features, target], axis=1).dropna()\n",
    "features = df.drop('SiteEnergyUse(kBtu)', axis=1)\n",
    "target = df['SiteEnergyUse(kBtu)']\n",
    "\n",
    "# Rescaling des variables numériques\n",
    "scaler = StandardScaler()\n",
    "features[['LargestPropertyUseTypeGFA', 'PropertyGFATotal', 'PropertyGFABuilding(s)']] = scaler.fit_transform(features[['LargestPropertyUseTypeGFA', 'PropertyGFATotal', 'PropertyGFABuilding(s)']])\n",
    "\n",
    "# Diviser les données en ensemble d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Liste des modèles à tester avec différents hyperparamètres\n",
    "models = [\n",
    "    Lasso()\n",
    "]\n",
    "\n",
    "best_model = None\n",
    "best_r2 = -float('inf')\n",
    "\n",
    "# Entraîner et évaluer les modèles\n",
    "for model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    \n",
    "    if r2 > best_r2:\n",
    "        best_r2 = r2\n",
    "        best_model = model\n",
    "\n",
    "print('Best Model:', best_model)\n",
    "print('Best R2 Score:', best_r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scalerenergy.pkl']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "# Enregistrer le modèle dans un fichier .pkl\n",
    "with open('modelenergy.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "import joblib\n",
    "joblib.dump(scaler, 'scalerenergy.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['LargestPropertyUseTypeGFA', 'PrimaryPropertyType_Distribution Center',\n",
      "       'PrimaryPropertyType_High-Rise Multifamily',\n",
      "       'PrimaryPropertyType_Hospital', 'PrimaryPropertyType_Hotel',\n",
      "       'PrimaryPropertyType_K-12 School', 'PrimaryPropertyType_Laboratory',\n",
      "       'PrimaryPropertyType_Large Office',\n",
      "       'PrimaryPropertyType_Low-Rise Multifamily',\n",
      "       'PrimaryPropertyType_Medical Office',\n",
      "       'PrimaryPropertyType_Mid-Rise Multifamily',\n",
      "       'PrimaryPropertyType_Mixed Use Property', 'PrimaryPropertyType_Office',\n",
      "       'PrimaryPropertyType_Other',\n",
      "       'PrimaryPropertyType_Refrigerated Warehouse',\n",
      "       'PrimaryPropertyType_Residence Hall', 'PrimaryPropertyType_Restaurant',\n",
      "       'PrimaryPropertyType_Retail Store',\n",
      "       'PrimaryPropertyType_Self-Storage Facility',\n",
      "       'PrimaryPropertyType_Senior Care Community',\n",
      "       'PrimaryPropertyType_Small- and Mid-Sized Office',\n",
      "       'PrimaryPropertyType_Supermarket / Grocery Store',\n",
      "       'PrimaryPropertyType_University', 'PrimaryPropertyType_Warehouse',\n",
      "       'PrimaryPropertyType_Worship Facility'],\n",
      "      dtype='object')\n",
      "R2 Score - Energy: 0.5970077719830454\n",
      "R2 Score - Emissions: 0.6991923453807781\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv('2016_Building_Energy_Benchmarking.csv')\n",
    "\n",
    "# Préparation des données\n",
    "features = df[['LargestPropertyUseTypeGFA', 'PrimaryPropertyType']].copy()\n",
    "targets = df[['SiteEnergyUse(kBtu)', 'TotalGHGEmissions']].copy()\n",
    "\n",
    "# Extraire tous les PrimaryPropertyType disponibles\n",
    "primary_property_types = features['PrimaryPropertyType'].unique()\n",
    "# print(primary_property_types)\n",
    "# Transformer les variables catégoriques en variables numériques\n",
    "features = pd.get_dummies(features, columns=['PrimaryPropertyType'])\n",
    "\n",
    "# Drop les NaN values du dataset\n",
    "df = pd.concat([features, targets], axis=1).dropna()\n",
    "features = df.drop(['SiteEnergyUse(kBtu)', 'TotalGHGEmissions'], axis=1)\n",
    "targets = df[['SiteEnergyUse(kBtu)', 'TotalGHGEmissions']]\n",
    "\n",
    "# Rescaling des variables numériques\n",
    "scaler = StandardScaler()\n",
    "features[['LargestPropertyUseTypeGFA']] = scaler.fit_transform(features[['LargestPropertyUseTypeGFA']])\n",
    "\n",
    "# Diviser les données en ensemble d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
    "print(X_train.columns)\n",
    "\n",
    "# Initialiser le modèle multi-output avec le modèle de régression souhaité (par exemple, Lasso)\n",
    "model = MultiOutputRegressor(Lasso())\n",
    "\n",
    "# Entraîner le modèle\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculer le R2 score pour chaque cible\n",
    "r2_energy = r2_score(y_test['SiteEnergyUse(kBtu)'], predictions[:, 0])\n",
    "r2_emissions = r2_score(y_test['TotalGHGEmissions'], predictions[:, 1])\n",
    "\n",
    "print('R2 Score - Energy:', r2_energy)\n",
    "print('R2 Score - Emissions:', r2_emissions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['multiscaler.pkl']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "# Enregistrer le modèle dans un fichier .pkl\n",
    "with open('multimodel.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "import joblib\n",
    "joblib.dump(scaler, 'multiscaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "column_list = ['LargestPropertyUseTypeGFA', 'PrimaryPropertyType_Distribution Center', 'PrimaryPropertyType_High-Rise Multifamily', 'PrimaryPropertyType_Hospital', 'PrimaryPropertyType_Hotel', 'PrimaryPropertyType_K-12 School', 'PrimaryPropertyType_Laboratory', 'PrimaryPropertyType_Large Office', 'PrimaryPropertyType_Low-Rise Multifamily', 'PrimaryPropertyType_Medical Office', 'PrimaryPropertyType_Mid-Rise Multifamily', 'PrimaryPropertyType_Mixed Use Property', 'PrimaryPropertyType_Office', 'PrimaryPropertyType_Other', 'PrimaryPropertyType_Refrigerated Warehouse', 'PrimaryPropertyType_Residence Hall', 'PrimaryPropertyType_Restaurant', 'PrimaryPropertyType_Retail Store', 'PrimaryPropertyType_Self-Storage Facility', 'PrimaryPropertyType_Senior Care Community', 'PrimaryPropertyType_Small- and Mid-Sized Office', 'PrimaryPropertyType_Supermarket / Grocery Store', 'PrimaryPropertyType_University', 'PrimaryPropertyType_Warehouse', 'PrimaryPropertyType_Worship Facility']\n",
    "\n",
    "# Enregistrer la liste des colonnes dans un fichier CSV\n",
    "with open('column_list.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(column_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score - Energy: 0.6289626021517745\n",
      "R2 Score - Emissions: 0.8156126694714736\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv('2016_Building_Energy_Benchmarking.csv')\n",
    "\n",
    "# Préparation des données\n",
    "features = df[['LargestPropertyUseTypeGFA', 'PrimaryPropertyType']].copy()\n",
    "targets = df[['SiteEnergyUse(kBtu)', 'TotalGHGEmissions']].copy()\n",
    "\n",
    "# Extraire tous les PrimaryPropertyType disponibles\n",
    "primary_property_types = features['PrimaryPropertyType'].unique()\n",
    "\n",
    "# Transformer les variables catégoriques en variables numériques\n",
    "features = pd.get_dummies(features, columns=['PrimaryPropertyType'])\n",
    "\n",
    "# Drop les NaN values du dataset\n",
    "df = pd.concat([features, targets], axis=1).dropna()\n",
    "features = df.drop(['SiteEnergyUse(kBtu)', 'TotalGHGEmissions'], axis=1)\n",
    "targets = df[['SiteEnergyUse(kBtu)', 'TotalGHGEmissions']]\n",
    "\n",
    "# Rescaling des variables numériques\n",
    "scaler = StandardScaler()\n",
    "features[['LargestPropertyUseTypeGFA']] = scaler.fit_transform(features[['LargestPropertyUseTypeGFA']])\n",
    "\n",
    "# Diviser les données en ensemble d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialiser le modèle multi-output avec le modèle de régression souhaité (ici, Random Forest)\n",
    "model = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "\n",
    "# Entraîner le modèle\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculer le R2 score pour chaque cible\n",
    "r2_energy = r2_score(y_test['SiteEnergyUse(kBtu)'], predictions[:, 0])\n",
    "r2_emissions = r2_score(y_test['TotalGHGEmissions'], predictions[:, 1])\n",
    "\n",
    "print('R2 Score - Energy:', r2_energy)\n",
    "print('R2 Score - Emissions:', r2_emissions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score - Energy: 0.6190364771294883\n",
      "R2 Score - Emissions: 0.7639295087476493\n",
      "Cross-validated R2 Score - Energy: 0.5297950884915024\n",
      "Cross-validated R2 Score - Emissions: 0.4369989645740942\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv('2016_Building_Energy_Benchmarking.csv')\n",
    "\n",
    "# Préparation des données\n",
    "features = df[['LargestPropertyUseTypeGFA', 'PrimaryPropertyType']].copy()\n",
    "targets = df[['SiteEnergyUse(kBtu)', 'TotalGHGEmissions']].copy()\n",
    "\n",
    "# Transformer les variables catégoriques en variables numériques\n",
    "features = pd.get_dummies(features, columns=['PrimaryPropertyType'])\n",
    "\n",
    "# Drop les NaN values du dataset\n",
    "df = pd.concat([features, targets], axis=1).dropna()\n",
    "features = df.drop(['SiteEnergyUse(kBtu)', 'TotalGHGEmissions'], axis=1)\n",
    "targets = df[['SiteEnergyUse(kBtu)', 'TotalGHGEmissions']]\n",
    "\n",
    "# Rescaling des variables numériques\n",
    "scaler = StandardScaler()\n",
    "features[['LargestPropertyUseTypeGFA']] = scaler.fit_transform(features[['LargestPropertyUseTypeGFA']])\n",
    "\n",
    "# Diviser les données en ensemble d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "# Définir les paramètres pour la recherche de grille\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialiser le modèle multi-output avec le modèle de régression souhaité (ici, Random Forest)\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "model = MultiOutputRegressor(GridSearchCV(rf, param_grid, cv=5))\n",
    "\n",
    "# Entraîner le modèle\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Faire des prédictions sur l'ensemble de test\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculer le R2 score pour chaque cible\n",
    "r2_energy = r2_score(y_test['SiteEnergyUse(kBtu)'], predictions[:, 0])\n",
    "r2_emissions = r2_score(y_test['TotalGHGEmissions'], predictions[:, 1])\n",
    "\n",
    "print('R2 Score - Energy:', r2_energy)\n",
    "print('R2 Score - Emissions:', r2_emissions)\n",
    "\n",
    "# Calculer le score de validation croisée pour chaque cible\n",
    "rf_energy = RandomForestRegressor(random_state=42)\n",
    "rf_emissions = RandomForestRegressor(random_state=42)\n",
    "\n",
    "cv_score_energy = cross_val_score(rf_energy, X_train, y_train['SiteEnergyUse(kBtu)'], cv=5)\n",
    "cv_score_emissions = cross_val_score(rf_emissions, X_train, y_train['TotalGHGEmissions'], cv=5)\n",
    "\n",
    "print('Cross-validated R2 Score - Energy:', np.mean(cv_score_energy))\n",
    "print('Cross-validated R2 Score - Emissions:', np.mean(cv_score_emissions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score: 0.5184040017675788\n",
      "Best Params: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 20}\n",
      "Best Score: 0.755303859394781\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "\n",
    "# Ignorer les avertissements\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv('2016_Building_Energy_Benchmarking.csv')\n",
    "\n",
    "# Préparation des données\n",
    "features = df[['LargestPropertyUseTypeGFA', 'PrimaryPropertyType']].copy()\n",
    "targets = df[['SiteEnergyUse(kBtu)', 'TotalGHGEmissions']].copy()\n",
    "\n",
    "# Transformer les variables catégoriques en variables numériques\n",
    "features = pd.get_dummies(features, columns=['PrimaryPropertyType'])\n",
    "\n",
    "# Drop les NaN values du dataset\n",
    "df = pd.concat([features, targets], axis=1).dropna()\n",
    "features = df.drop(['SiteEnergyUse(kBtu)', 'TotalGHGEmissions'], axis=1)\n",
    "targets = df[['SiteEnergyUse(kBtu)', 'TotalGHGEmissions']]\n",
    "\n",
    "# Rescaling des variables numériques\n",
    "scaler = StandardScaler()\n",
    "features[['LargestPropertyUseTypeGFA']] = scaler.fit_transform(features[['LargestPropertyUseTypeGFA']])\n",
    "\n",
    "# Diviser les données en ensemble d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "# Définir les paramètres pour la recherche de grille\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(model, param_distributions=rf_param_grid, n_iter=10, cv=5)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "scores = [r2_score(y_test[column], predictions[:, i]) for i, column in enumerate(y_test.columns)]\n",
    "best_score = np.mean(scores)\n",
    "\n",
    "# Cross validation score\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5)\n",
    "cv_score_mean = np.mean(cv_scores)\n",
    "\n",
    "print(f\"Cross Validation Score: {cv_score_mean}\")\n",
    "print(f\"Best Params: {best_params}\")\n",
    "print(f\"Best Score: {best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m model \u001b[39m=\u001b[39m RandomForestRegressor(random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m     47\u001b[0m random_search \u001b[39m=\u001b[39m RandomizedSearchCV(model, param_distributions\u001b[39m=\u001b[39mrf_param_grid, n_iter\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m---> 48\u001b[0m random_search\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     50\u001b[0m best_model \u001b[39m=\u001b[39m random_search\u001b[39m.\u001b[39mbest_estimator_\n\u001b[0;32m     51\u001b[0m best_params \u001b[39m=\u001b[39m random_search\u001b[39m.\u001b[39mbest_params_\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1768\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1766\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1767\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1768\u001b[0m     evaluate_candidates(\n\u001b[0;32m   1769\u001b[0m         ParameterSampler(\n\u001b[0;32m   1770\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[0;32m   1771\u001b[0m         )\n\u001b[0;32m   1772\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    823\u001b[0m         clone(base_estimator),\n\u001b[0;32m    824\u001b[0m         X,\n\u001b[0;32m    825\u001b[0m         y,\n\u001b[0;32m    826\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    827\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    828\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    829\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    830\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    831\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    833\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    834\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    835\u001b[0m     )\n\u001b[0;32m    836\u001b[0m )\n\u001b[0;32m    838\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;49;00m func, args, kwargs \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mitems]\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:686\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    684\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    685\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 686\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[0;32m    688\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    690\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:473\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    462\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    463\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    464\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    465\u001b[0m ]\n\u001b[0;32m    467\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    470\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    471\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 473\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    474\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    475\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    476\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    477\u001b[0m )(\n\u001b[0;32m    478\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    479\u001b[0m         t,\n\u001b[0;32m    480\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    481\u001b[0m         X,\n\u001b[0;32m    482\u001b[0m         y,\n\u001b[0;32m    483\u001b[0m         sample_weight,\n\u001b[0;32m    484\u001b[0m         i,\n\u001b[0;32m    485\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    486\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    487\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    488\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    489\u001b[0m     )\n\u001b[0;32m    490\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    491\u001b[0m )\n\u001b[0;32m    493\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;49;00m func, args, kwargs \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mitems]\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:184\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    182\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 184\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    185\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1247\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   1219\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1220\u001b[0m \n\u001b[0;32m   1221\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1244\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1247\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m   1248\u001b[0m         X,\n\u001b[0;32m   1249\u001b[0m         y,\n\u001b[0;32m   1250\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1251\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m   1252\u001b[0m     )\n\u001b[0;32m   1253\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\loren\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:379\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    370\u001b[0m         splitter,\n\u001b[0;32m    371\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    377\u001b[0m     )\n\u001b[1;32m--> 379\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[0;32m    381\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    382\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "\n",
    "# Ignorer les avertissements\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv('2016_Building_Energy_Benchmarking.csv')\n",
    "\n",
    "# Préparation des données\n",
    "features = df[['LargestPropertyUseTypeGFA', 'PrimaryPropertyType']].copy()\n",
    "targets = df[['SiteEnergyUse(kBtu)', 'TotalGHGEmissions']].copy()\n",
    "\n",
    "# Transformer les variables catégoriques en variables numériques\n",
    "features = pd.get_dummies(features, columns=['PrimaryPropertyType'])\n",
    "\n",
    "# Drop les NaN values du dataset\n",
    "df = pd.concat([features, targets], axis=1).dropna()\n",
    "features = df.drop(['SiteEnergyUse(kBtu)', 'TotalGHGEmissions'], axis=1)\n",
    "targets = df[['SiteEnergyUse(kBtu)', 'TotalGHGEmissions']]\n",
    "\n",
    "# Rescaling des variables numériques\n",
    "scaler = StandardScaler()\n",
    "features[['LargestPropertyUseTypeGFA']] = scaler.fit_transform(features[['LargestPropertyUseTypeGFA']])\n",
    "\n",
    "# Diviser les données en ensemble d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "# Définir les paramètres pour la recherche de grille\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(model, param_distributions=rf_param_grid, n_iter=100, cv=5)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "scores = [r2_score(y_test[column], predictions[:, i]) for i, column in enumerate(y_test.columns)]\n",
    "best_score = np.mean(scores)\n",
    "\n",
    "# Cross validation score\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5)\n",
    "cv_score_mean = np.mean(cv_scores)\n",
    "\n",
    "print(f\"Cross Validation Score: {cv_score_mean}\")\n",
    "print(f\"Best Params: {best_params}\")\n",
    "print(f\"Best Score: {best_score}\")\n",
    "\n",
    "# Enregistrer le modèle dans un fichier .pkl\n",
    "with open('best_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "# Enregistrer le scaler dans un fichier .pkl\n",
    "joblib.dump(scaler, 'best_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score: 0.5172601774320562\n",
      "Best Params: {'n_estimators': 500, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 20}\n",
      "Best Score: 0.7510550379504478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['best_scaler.pkl']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "\n",
    "# Ignorer les avertissements\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv('2016_Building_Energy_Benchmarking.csv')\n",
    "\n",
    "# Préparation des données\n",
    "features = df[['LargestPropertyUseTypeGFA', 'PrimaryPropertyType']].copy()\n",
    "targets = df[['SiteEnergyUse(kBtu)', 'TotalGHGEmissions']].copy()\n",
    "\n",
    "# Transformer les variables catégoriques en variables numériques\n",
    "features = pd.get_dummies(features, columns=['PrimaryPropertyType'])\n",
    "\n",
    "# Drop les NaN values du dataset\n",
    "df = pd.concat([features, targets], axis=1).dropna()\n",
    "features = df.drop(['SiteEnergyUse(kBtu)', 'TotalGHGEmissions'], axis=1)\n",
    "targets = df[['SiteEnergyUse(kBtu)', 'TotalGHGEmissions']]\n",
    "\n",
    "# Transformation logarithmique de la variable 'LargestPropertyUseTypeGFA'\n",
    "features['LargestPropertyUseTypeGFA'] = np.log(features['LargestPropertyUseTypeGFA'])\n",
    "\n",
    "# Rescaling des variables numériques\n",
    "scaler = StandardScaler()\n",
    "features[['LargestPropertyUseTypeGFA']] = scaler.fit_transform(features[['LargestPropertyUseTypeGFA']])\n",
    "\n",
    "# Diviser les données en ensemble d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "# Définir les paramètres pour la recherche de grille\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(model, param_distributions=rf_param_grid, n_iter=1, cv=5)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "scores = [r2_score(y_test[column], predictions[:, i]) for i, column in enumerate(y_test.columns)]\n",
    "best_score = np.mean(scores)\n",
    "\n",
    "# Cross validation score\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5)\n",
    "cv_score_mean = np.mean(cv_scores)\n",
    "\n",
    "print(f\"Cross Validation Score: {cv_score_mean}\")\n",
    "print(f\"Best Params: {best_params}\")\n",
    "print(f\"Best Score: {best_score}\")\n",
    "\n",
    "# Enregistrer le modèle dans un fichier .pkl\n",
    "with open('best_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "# Enregistrer le scaler dans un fichier .pkl\n",
    "joblib.dump(scaler, 'best_scaler.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Score: 0.449093957849767\n",
      "Best Params: {'n_estimators': 400, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 10}\n",
      "Best Score: 0.665673650573968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['best_scaler.pkl']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "\n",
    "# Ignorer les avertissements\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv('2016_Building_Energy_Benchmarking.csv')\n",
    "\n",
    "# Préparation des données\n",
    "features = df[['LargestPropertyUseTypeGFA', 'PrimaryPropertyType']].copy()\n",
    "targets = df[['SiteEnergyUse(kBtu)', 'TotalGHGEmissions']].copy()\n",
    "\n",
    "# Transformer les variables catégoriques en variables numériques\n",
    "features = pd.get_dummies(features, columns=['PrimaryPropertyType'])\n",
    "\n",
    "# Drop les NaN values du dataset\n",
    "df = pd.concat([features, targets], axis=1).dropna()\n",
    "features = df.drop(['SiteEnergyUse(kBtu)', 'TotalGHGEmissions'], axis=1)\n",
    "targets = df[['SiteEnergyUse(kBtu)', 'TotalGHGEmissions']]\n",
    "\n",
    "# Save the columns of the DataFrame\n",
    "with open('columns.pkl', 'wb') as f:\n",
    "    pickle.dump(features.columns.tolist(), f)\n",
    "\n",
    "# Transformation logarithmique de la variable 'LargestPropertyUseTypeGFA'\n",
    "features['LargestPropertyUseTypeGFA'] = np.log(features['LargestPropertyUseTypeGFA'])\n",
    "\n",
    "# Rescaling des variables numériques\n",
    "scaler = StandardScaler()\n",
    "features[['LargestPropertyUseTypeGFA']] = scaler.fit_transform(features[['LargestPropertyUseTypeGFA']])\n",
    "\n",
    "# Diviser les données en ensemble d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "# Définir les paramètres pour la recherche de grille\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(model, param_distributions=rf_param_grid, n_iter=1, cv=5)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "scores = [r2_score(y_test[column], predictions[:, i]) for i, column in enumerate(y_test.columns)]\n",
    "best_score = np.mean(scores)\n",
    "\n",
    "# Cross validation score\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5)\n",
    "cv_score_mean = np.mean(cv_scores)\n",
    "\n",
    "print(f\"Cross Validation Score: {cv_score_mean}\")\n",
    "print(f\"Best Params: {best_params}\")\n",
    "print(f\"Best Score: {best_score}\")\n",
    "\n",
    "# Enregistrer le modèle dans un fichier .pkl\n",
    "with open('best_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "# Enregistrer le scaler dans un fichier .pkl\n",
    "joblib.dump(scaler, 'best_scaler.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero or negative values in 'LargestPropertyUseTypeGFA': 0\n",
      "Some scaled values: 0    0.726587\n",
      "1    0.664610\n",
      "2    3.242804\n",
      "3    0.297358\n",
      "4    1.117587\n",
      "Name: LargestPropertyUseTypeGFA, dtype: float64\n",
      "Cross Validation Score: 0.5258021032620898\n",
      "Best Params: {'n_estimators': 100, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 10}\n",
      "Best Score: 0.7402157485522256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['best_scaler.pkl']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "\n",
    "# Ignorer les avertissements\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Charger les données\n",
    "df = pd.read_csv('2016_Building_Energy_Benchmarking.csv')\n",
    "\n",
    "# Préparation des données\n",
    "features = df[['LargestPropertyUseTypeGFA', 'PrimaryPropertyType']].copy()\n",
    "targets = df[['SiteEnergyUse(kBtu)', 'TotalGHGEmissions']].copy()\n",
    "\n",
    "# Save the original columns before one-hot encoding\n",
    "original_columns = features.columns.tolist()\n",
    "\n",
    "# Transformer les variables catégoriques en variables numériques\n",
    "features = pd.get_dummies(features, columns=['PrimaryPropertyType'])\n",
    "\n",
    "# Drop les NaN values du dataset\n",
    "df = pd.concat([features, targets], axis=1).dropna()\n",
    "features = df.drop(['SiteEnergyUse(kBtu)', 'TotalGHGEmissions'], axis=1)\n",
    "targets = df[['SiteEnergyUse(kBtu)', 'TotalGHGEmissions']]\n",
    "\n",
    "# Save the columns of the DataFrame\n",
    "with open('columns.pkl', 'wb') as f:\n",
    "    pickle.dump(features.columns.tolist(), f)\n",
    "\n",
    "# Save the original columns before one-hot encoding\n",
    "with open('original_columns.pkl', 'wb') as f:\n",
    "    pickle.dump(original_columns, f)\n",
    "\n",
    "# Check for any zero or negative values in 'LargestPropertyUseTypeGFA'\n",
    "print(\"Zero or negative values in 'LargestPropertyUseTypeGFA':\", features['LargestPropertyUseTypeGFA'].le(0).sum())\n",
    "\n",
    "# Transformation logarithmique de la variable 'LargestPropertyUseTypeGFA'\n",
    "features['LargestPropertyUseTypeGFA'] = np.log(features['LargestPropertyUseTypeGFA'])\n",
    "\n",
    "# Rescaling des variables numériques\n",
    "scaler = StandardScaler()\n",
    "features[['LargestPropertyUseTypeGFA']] = scaler.fit_transform(features[['LargestPropertyUseTypeGFA']])\n",
    "\n",
    "# Print some scaled values for inspection\n",
    "print(\"Some scaled values:\", features['LargestPropertyUseTypeGFA'].head())\n",
    "\n",
    "# Diviser les données en ensemble d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Définir les paramètres pour la recherche de grille\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(model, param_distributions=rf_param_grid, n_iter=100, cv=5)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "predictions = best_model.predict(X_test)\n",
    "scores = [r2_score(y_test[column], predictions[:, i]) for i, column in enumerate(y_test.columns)]\n",
    "best_score = np.mean(scores)\n",
    "\n",
    "# Cross validation score\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5)\n",
    "cv_score_mean = np.mean(cv_scores)\n",
    "\n",
    "print(f\"Cross Validation Score: {cv_score_mean}\")\n",
    "print(f\"Best Params: {best_params}\")\n",
    "print(f\"Best Score: {best_score}\")\n",
    "\n",
    "# Enregistrer le modèle dans un fichier .pkl\n",
    "with open('best_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "\n",
    "# Enregistrer le scaler dans un fichier .pkl\n",
    "joblib.dump(scaler, 'best_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5644161.9      139.19]]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "with open('best_model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "scaler = joblib.load('best_scaler.pkl')\n",
    "\n",
    "# Load the model's columns\n",
    "with open('columns.pkl', 'rb') as f:\n",
    "    model_columns = pickle.load(f)\n",
    "\n",
    "# Create a new row of data with all zeros\n",
    "new_data = pd.DataFrame(columns=model_columns)\n",
    "new_data.loc[0] = [0 for _ in range(len(model_columns))]\n",
    "\n",
    "# Fill in the data for your prediction\n",
    "new_data['PrimaryPropertyType_Hotel'] = 1\n",
    "\n",
    "# The initial value before transformations\n",
    "initial_value = 88434  # change this to your actual value\n",
    "\n",
    "# Apply inverse transformations\n",
    "log_value = np.log(initial_value)\n",
    "scaled_value = scaler.transform([[log_value]])[0][0]\n",
    "\n",
    "new_data['LargestPropertyUseTypeGFA'] = scaled_value\n",
    "\n",
    "predictions = model.predict(new_data)\n",
    "\n",
    "# Set options for printing\n",
    "np.set_printoptions(suppress=True, precision=2)\n",
    "\n",
    "print(predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
